Entity - based Neural Local Coherence Modeling
In this paper , we propose an entity - based neural local coherence model which is linguistically more sound than previously proposed neural coherence models . Recent neural coherence models encode the input document using large - scale pretrained language models . Hence their basis for computing local coherence are words and even sub - words . An analysis of their output shows that these models frequently compute coherence on the basis of connections between ( sub-)words which , from a linguistic perspective , should not play a role . Still , these models achieve state - of - the - art performance in several end applications . In contrast to these models , we compute coherence on the basis of entities by constraining the input to noun phrases and proper names . This provides us with an explicit representation of the most important items in sentences leading to the notion of focus . This brings our model linguistically in line with pre - neural models of computing coherence . It also gives us better insight into the behaviour of the model thus leading to better explainability . Our approach is also in accord with a recent study ( O'Connor and Andreas , 2021 ) , which shows that most usable information is captured by nouns and verbs in transformer - based language models . We evaluate our model on three downstream tasks showing that it is not only linguistically more sound than previous models but also that it outperforms them in end applications 1 .
Introduction
Coherence describes the semantic relation between elements of a text . It recognizes how well a text is organized to convey the information to the reader effectively . Modeling coherence can be beneficial to any system which needs to process a text .
Example Sentence 1 Mr. Specter , seeming exasperated , said in an interview Thursday .
Focus candidates captured by XLNet " _ said " , " _ in " , " day " , " _ interview " , " _ , " , " er " , " _ an " , " th " , " s " , " _ exasperated " , ... , " spect " Example Sentence 2 At the same time , unadvertised products may have almost identical ingredients but less namerecognition .
Focus candidates captured by XLNet " _ name " , " ition " , " _ products " , " - " , " _ un " , " _ may " , " _ less " , " _ ingredients " , " _ have " , ... , " _ same " Table 1 : The pretrained language model , XLNet Yang et al . ( 2019 ) , captures undesirable ( sub-)words as focus ( Jeon and Strube , 2020 ) . The sub - words are sorted by their attention scores in descending order . In the first example , " Thursday " is split into four : " th " , " ur " , " s " , and " day " . In the second example , some sub - words , such as " ition " , might be beneficial in their vector space but the model might exploit spurious information .
Recent neural coherence models ( Mesgar and Strube , 2018;Moon et al . , 2019 ) encode the input document using large - scale pretrained language models ( Peters et al . , 2018 ) . These neural models compute local coherence , semantic relations between items in adjacent sentences , on the basis of words and even sub - words .
However , it has been unclear on which basis these models compute local coherence . Jeon and Strube ( 2020 ) present a neural coherence model , which allows to interpret focus information for the first time . Their investigation reveals that neural models , adopting large - scale pretrained language models , compute coherence on the basis of connections between any ( sub-)words or function words ( Table 1,11 ) . In these cases , the model might capture the focus based on spurious information . While such a model might reach or set the state of the art in some end applications , it will do so for the wrong reasons from a linguistic perspective .
This problem did not appear with pre - neural models , since they compute coherence on the basis of entities . Early work about pronoun and anaphora resolution by Sidner ( 1981Sidner ( , 1983 assumes that there is one single salient entity in a sentence , its focus , which serves as a preferred antecedent for anaphoric expressions . Centering theory ( Joshi and Weinstein , 1981;Grosz et al . , 1995 ) builds on these insights and introduces an algorithm for tracking changes in focus . Centering theory serves as basis for many researchers to develop systems computing local coherence by approximating entities ( Barzilay and Lapata 2008;Feng and Hirst 2012;Guinaudeau and Strube 2013 , inter alia ) .
In this paper , we propose a neural coherence model which is linguistically more sound than previously proposed neural coherence models . We compute coherence on the basis of entities by constraining our model to capture focus on noun phrases and proper names . This provides us with an explicit representation of the most important items in sentences , leading to the notion of focus . This brings our model linguistically in line with pre - neural models of coherence .
Our approach is not only linguistically more sound but also is in accord with a recent empirical study by O'Connor and Andreas ( 2021 ) who investigate what contextual information contributes to accurate predictions in transformer - based language models . Their experiments show that most usable information is captured by nouns and verbs . Their findings suggest that we can design better neural models by focusing on specific context words . Our work follows their findings by modeling entitybased coherence in an end - to - end framework to improve a neural coherence model .
Our model integrates a local coherence module with a component which takes context into account . Our model first encodes a document using a pretrained language model and identifies entities using a linguistic parser . The local coherence module captures the most related representations of entities between adjacent sentences , the local focus . Then it tracks the changes of local foci . The second component captures the context of a text by averaging sentence representations .
We evaluate our model on three downstream tasks : automated essay scoring ( AES ) , assessing writing quality ( AWQ ) , and assessing discourse coherence ( ADC ) . AES and AWQ determine text quality for a given text , aiming to replicate human scoring results . Since coherence is an essential factor in assessing text quality , many previous coherence models are evaluated on AES and AWQ . ADC evaluates coherence models on informal texts such as emails and online reviews . In our evaluation , our model achieves state - of - the - art performance .
We also perform a series of analyses to investigate how our model works . Our analyses show that capturing focus on entities gives us better insight into the behaviour of the model , leading to better explainability . Using this information , we examine statistical differences of texts assigned to different qualities . From the perspective of local coherence , we find that texts of higher quality are neither semantically too consistent nor too variant . Finally , we inspect error cases to examine how our model works differently compared to previous models .
Related Work
Entity - based modeling has been the prevailing approach to model coherence in pre - neural models . The entity grid is its most well - known implementation ( Barzilay and Lapata , 2008 ) . It represents entities in a two - dimensional array to track their transitions between sentences . Many variations have been proposed to improve this model , e.g. , projecting the grid into a graph representation ( Guinaudeau and Strube , 2013 ) or converting the grid to a neural model ( Tien Nguyen and Joty , 2017 ) .
However , the neural version of the entity grid ( Tien Nguyen and Joty , 2017 ) has two limitations . First , Lai and Tetreault ( 2018 ) state that entity grids applied to downstream tasks are often extremely sparse . In their evaluation , it is difficult to find meaningful entity transitions between sentences in the grids . Accordingly , this model performs worse than other neural models . More importantly , this neural model can not provide any clues of how this model works since Tien Nguyen and Joty ( 2017 ) apply a convolutional layer on the entity grid . The feature map of the convolutional layer is not interpretable . They can not examine which entity is assigned more importance than others by their model . In contrast , we constrain our model to capture focus on entities using noun phrases . Then our model tracks the changes of focus . Hence , it provides us with an interpretable focus ( Section 5 ) .
More recently , Moon et al . ( 2019 ) propose a neural coherence model to exploit both local and structural aspects . They evaluate their model on an arti - ficial task only , the shuffle test , which determines whether sentences in a document are shuffled or not . However , recent studies ( Pishdad et al . , 2020 ) claim that this artificial task is not suitable to evaluate coherence models . Lai and Tetreault ( 2018 ) show that the neural coherence models , which achieve the best performance on this task , do not outperform non - neural models on downstream tasks . More recently , Mohiuddin et al . ( 2021 ) find a weak correlation between the model performance in artificial tasks and downstream tasks . In our evaluation , we compare Moon et al . ( 2019 ) with ours in an artificial task as well as in three downstream tasks . Moon et al . ( 2019 ) perform the best in the artificial task , but do not outperform our model in three downstream tasks ( Section 4 ) .
Our Model
Figure 1 presents the architecture of our model . We first introduce our entity representation and sentence encoding using a pretrained language model . Next , we describe a novel local coherence model . We then combine the two representations of local coherence and the context vector , simply averaged sentence representations . Finally , we apply a feedforward network to produce a score label .
Sentence Encoding
We use a pretrained language model ( Yang et al . , 2019 ) to encode sentences . XLNet learns bidirectional contexts by maximizing expected likelihood using an autoregressive training objective . Hence it allows to capture the focus in sentences . XLNet outperforms other language models in tasks which require processing long texts .
Recent work investigates that pretrained language models learn linguistic features that are helpful for language understanding ( Tenney et al . , 2019;Warstadt et al . , 2020 ) . Inspired by this , we encode two adjacent sentences at once to capture discourse features , such as coreference relations . In this strategy , items are encoded twice except the items included in the first and the last sentence . We interpolate items encoded twice to consider context with regard to the preceding and succeeding sentence .
We encode an input document using XLNet to obtain word representations . Sentence representations are means of all word representations in a sentence . We then feed sentence representations and the noun phrase representations into the the coherence modules .   In formal definitions , let E e = [ h ( e , i,1 ) , ... , h ( e , i , m ) , h ( e , i+1,1 ) , ... , h ( e , i+1,m ) ] denote the output of encoding , where e indicates the index of encoding , and m indicates the index of a subword ( w ) in the sentence ( s i ) . h indicates the encoded representation of w. This encoding output includes the encoded representations of s i and s i+1 since we encode two adjacent sentences at once . Likewise , E e+1 = [ h ( e+1,i+1,1 ) , ... , h ( e+1,i+2,m ) ] is the output in the next encoding , and it includes the encoded representations of s i+1 and s i+2 . Then , the encoded representation of s i+1 is a sequence of ih ( i+1,m ) = avg(h ( e , i+1,m ) , h ( e+1,i+1,m ) ) , which is the interpolated representation of s i+1 in the two encoding stages ( e and e + 1 ) . We iterate this process to encode all adjacent sentences .
Entity Identification
Pretrained language models encode sequences as sub - words , but to our knowledge , there is no linguistic parser using sub - words as input . Hence , we use a linguistic parser to identify noun phrases in each sentence separately . Kitaev and Klein ( 2018 ) present a neural constituency parser which determines the syntactic structure of a sentence . To identify noun phrases and proper names , we ap - ply this parser to the original sentences , then map parsed constituents to sub - word tokens .
Since pretrained language models do not have the means to represent phrase meaning composition , we average sub - word representations for phrases which consists of multiple sub - words . While this implementation does not capture the complex meaning of phrases , Yu and Ettinger ( 2020 ) report that it shows higher correlation with human annotations than using the last word of phrases , assuming that the last word of a phrase is its head .
Let N P i = [ np i,1 , np i,2 , ... , np i , j ] denote a sequence of noun phases ( np ) in the ith sentence , and j indicates the index of a noun phrase in the sentence . Each representation of a noun phrase is obtained as np i , j = avg(ih i,1 , ... , ih i , k ) , where ih i , k indicates the subword tokens contributing to the same entity .
Local Coherence Module
We compare the semantic representations of noun phrases between adjacent sentences . The two most similar representations of noun phrases are taken as local focus of the respective sentences . These two representations are averaged to capture the common context . We use cosine similarity to measure semantic similarity .
We notice that some sentences do not include noun phrases , approximately 3.5 % in the three datasets used in our evaluation . This mostly occurs when some words are omitted as in cases of ellipsis ( Hardt and Romero , 2004 ) . In such cases , we maintain the focus of the previous sentence to preserve the context .
A depthwise convolutional layer is applied to the local focus to record its transitions . Unlike a typical convolutional layer , the depthwise convolutional layer captures the patterns of semantic changes between different time - steps for the same spatial information ( Chollet , 2017 ) . In our model , this layer captures the semantic changes between local foci considering the context but on the same spatial dimension of each focus . Hence , it does not hurt the explainability of our model . We use the lightweight depthwise convolutional layer ( Wu et al . , 2019 ) .
Then we update the representations of local foci to track the semantic changes between them . We use the Tree - Transformer which updates its hidden representations by inducing a tree - structure from a document . It generates constituent priors by calculating neighboring attention which represents the probability of whether adjacent items are in the same constituent . The constituent priors constrain the self - attention of the transformer to follow the induced structure .
Finally , we apply document attention to produce the weighted sum of all the updated local focus representations . The document attention identifies relative weights of updated representations which enables our model to handle any document length .
In formal descriptions , let mnp l , i denote the representations of two noun phrases which have the highest cosine similarity scores between the ith and i + 1th sentence . Then , we define LocalF = [ localf 1 , ... , localf l ] , where localf l is an averaged representation of mnp l , i and mnp l , i+1 . It represents the sequence of local foci between the ith and i + 1th sentence , and l indicates the index of the local focus in the document . Finally , the local coherence representation is obtained as lcr = doc_attn(tree_trans(dconv(LocalF ) ) ) where dconv indicates the depthwise convolutional layer , tree_trans indicates the Tree - Transformer , and doc_attn indicates the document attention .
Experiments
Implementation Details
We implement our model using the PyTorch library and use the Stanford Stanza library 2 for sentence tokenization . We employ XLNet for the pretrained language model . For the baselines which do not employ a pretrained language model ( Dong et al . , 2017;Mesgar and Strube , 2018 ) , GloVe is employed for word embeddings , trained on Google News ( Pennington et al . , 2014 ) ( see Appendix A for more details ) .
To compare baselines within the same framework , we re - implement all of them in PyTorch . We then use our re - implementation to report the performance of models with 10 runs with different random seeds . We verify statistical significance ( p - value<0.01 ) with both a one - sample t - test , which verifies the reproducibility of the performance of each model , and a two - sample t - test , which verifies that the performance of our model is statistically significantly different from other models .
Within the same framework we compare the size of models used in our experiments . Our neural model uses a number of parameters comparable to the state of the art , the transformer - based model
Baselines : Neural Coherence Models
In all three downstream tasks , we compare our model against recent neural coherence models . First , Mesgar and Strube ( 2018 ) propose a neural local coherence model , based on Centering theory . This model connects the most related states of a Recurrent Neural Network , then represents the coherence patterns using semantic distances between the states . Second , Moon et al . ( 2019 ) propose a unified neural coherence model to consider local and structural aspects . This model consists of two modules when they employ a pretrained language model ( Peters et al . , 2018 ): a module of inter - sentence relations using a bilinear layer and a topic structure module applying a depth - wise convolutional layer to the sentence representations . To ensure fair comparison , XLNet is employed for this model as well , instead of ELMo ( Peters et al . , 2018 ) . More recently , Jeon and Strube ( 2020 ) propose a neural coherence model approximating the structure of a document by connecting linguistic insights and a pretrained language model . This model consists of two sub - modules . First , a discourse segment parser constructs structural relationships for discourse segments by tracking the changes of focus between discourse segments . Second , a structure - aware transformer updates sentence representation using this structural information .
Artificial Task : Shuffle Test
We first evaluate our model on the artificial setup , the shuffle test , used in earlier works ( Table 2 ) . We follow the setup used in Lai and Tetreault ( 2018 ) . In this setup , our model outperforms a simple neural model relying on the pretrained language model . Moon et al . ( 2019   This result is not surprising . There is a line of recent work which shows that this setup is not capable of evaluating coherence models from diverse perspectives . Laban et al . ( 2021 ) show that employing fine - tuned language models simply achieves a near - perfect accuracy on this setup . O'Connor and Andreas ( 2021 ) measure usable information by selectively ablating lexical and structural information in transformer - based language models . Their findings show that prediction accuracy depends on information about local word co - occurrences , but not word order or global position . We suspect that exploiting all information of a sentence is sufficient for shuffle tests to capture patterns to distinguish whether sentences in a document are shuffled or not . Based on these findings , we evaluate our model on three downstream tasks used for evaluating coherence models , automated essay scoring , assessing writing quality , and assessing discourse coherence . We advise future work not to evaluate coherence models on the artificial setup solely .
Automated Essay Scoring ( AES )
Dataset . To evaluate the coherence models on AES , we evaluate them on the Test of English as a Foreign Language ( TOEFL ) dataset ( Blanchard et al . , 2013 ) . While the Automated Student Assessment Prize ( ASAP ) dataset 3 is frequently used for AES , TOEFL has a generally higher quality of essays compared to essays in ASAP . The prompts in ASAP are written by students in grade levels 7 to 10 of US middle schools . Many essays in ASAP consist of only a few sentences . In contrast , the prompts in TOEFL are submitted for the standard English test for the entrance to universities by nonnative students . The prompts in TOEFL do not vary so much , the student population is more controlled , and essays have a similar length .
Evaluation Setup . We follow the evaluation setup of previous work on AES ( Taghipour and Ng , 2016 ) . For TOEFL , we evaluate performance with accuracy for the 3 - class classification problem with 5 - fold cross - validation . We use the same split for the cross - validation , used by Jeon and Strube ( 2020 ) . The cross - entropy loss is deployed for training . The ADAM optimizer is used for our model with a learning rate of 0.003 . We evaluate performance for 25 epochs on the validation set with a mini - batch size of 32 . The model which reaches the   best accuracy on the validation set is then applied to the test set .
Baselines . We compare against Dong et al . ( 2017 ) , a neural model proposed for AES . They present a model consisting of a convolutional layer , followed by a recurrent layer , and an attention layer ( Bahdanau et al . , 2015 ) between the adjacent tokens .
Results .    1SentEnc 56.2 ( 0.5 ) 61.0 ( 0.4 ) 53.6 ( 0.5 ) 56.6 ( 0.4 ) 56.9 Jeon and Strube ( 2020)-1SentEnc 56.4 ( 0.6 ) 62.5 ( 0.9 ) 54.5 ( 0.4 ) 56.9 ( 0.3 ) 57.6 Jeon and Strube ( 2020)-2SentsEnc 57.2 ( 0.5 ) 63.0 ( 0.4 ) 54.4 ( 0.4 ) 56.9 ( 0.2 ) 57.9 Our Model 58.4 ( 0.2 ) 64.2 ( 0.4 ) 55.3 ( 0.3 ) 57.3 ( 0.2 ) 58.9   ( Lai and Tetreault , 2018 ) . We perform 10 - fold crossvalidation , use accuracy as evaluation measure on the 3 - class classification , and use the cross - entropy loss function .
Baselines . Li and Jurafsky ( 2017 ) propose a neural model based on cliques , that are sets of adjacent sentences . This model uses the cliques taken from the original article as a positive label and uses cliques with randomly permutated ones as a negative label . Lai and Tetreault ( 2018 ) show that a simple neural model which uses paragraph information outperforms previous models on GCDC .
Results . We also suspect that human annotators recognize important entities in the texts , such as the name of a person in the US government .
Ablation Study
Since our model consists of several components , we examine the influence of each component on the performance of the AES task . Specifically , we first examine the influence of our local coherence module . Then we examine the influence of the Tree - Transformer compared to a naive Transformer . Lastly , we examine the influence of the depth - wise convolutional layer deployed ahead of the Tree - Transformer .
Table 7 shows that each component contributes to the performance meaningfully while the depthwise convolutional layer increases the performance slightly . This suggests that we could design a better component in future work to capture semantic transitions between local foci .     ( Jeon and Strube , 2020 ) and the focus captured on noun phrases using our model . The essays submitted to prompt 1 in TOEFL and NYT article ID 1516415 ( see Table 14 in the Appendix D for more details ) .
Analysis
Capturing Focus Using Entities
In Centering theory , the focus is described as the most important item in a sentence . Jeon and Strube ( 2020 ) capture the focus using attention scores and analyze texts assigned to different qualities using this focus . They state that the focus is difficult to interpret when it is composed of sub - words . To investigate this further , we compare the focus captured on any ( sub-)words and the focus constrained to entities . Table 6 indicates that constraining focus to entities leads to better explainability , in particular on NYT . For example , in the NYT-1516415 news article about String theory , a subword of " ein " is not an interpretable focus . It may , however , include useful information in the vector space for a neural model . In contrast , our entity - based model leads to better explainability . Instead of " ein " , it provides the more interpretable focus , " Einstein " , a theoretical physicist . In TOEFL , " broad knowledge " is a more interpretable focus than a focus consisting of the single subword tokens , " broad " . Table 6 also shows that our model mainly uses pronouns , and noun phrases are playing an important role to represent focus . This suggests that further investigation is needed to understand how language models work on pronouns to process a text .
Local Coherence Patterns
Using interpretable focus information , we investigate differences in focus transitions of texts assigned to different scores . Motivated by the definition of the continue and the shift transition in Centering theory , we define semantic consistency which represents the degree of semantic changes between local foci . Two adjacent sentences are semantically consistent when the semantic simi - larity ( sim i ) between the local foci ( lf ) is higher than a semantic threshold ( θ sem;score ) . This threshold is determined as the average of semantic similarities between local foci of adjacent sentences in texts assigned the same score . Otherwise , a semantic transition ( st ) occurs between the local foci : st i = 1 if sim i < θ sem;score . Finally , the semantic consistency ( SC ) is defined as follows :
SC = 1 − ( count(st i ) /|lf | ) .
Figure 2 illustrates the semantic consistency on TOEFL , and Table 8 shows the statistics of the semantic consistency on texts assigned to different scores . Texts assigned a high score show lower semantic consistency on average . This indicates that texts of higher quality are overall more semantically variant than texts of lower quality . Additionally , we observe that texts assigned a low score show significantly larger proportions of an extreme level of semantic consistency . We define the extreme level as either texts whose semantic consistency is lower than 5 % , indicating texts are highly variant , or texts whose semantic consistency is higher than 75 % , indicating texts are highly consistent . Hence , these findings indicate that texts of lower quality are semantically too variant or too consistent . Texts of higher quality are neither too variant nor too consistent .
We next inspect the focus of texts assigned to different scores ( see Table 15,16 , and 17 in the Appendix D for more details ) . This shows that pronouns more frequently indicate the local focus in texts of lower quality than in texts of higher quality . The essays in TOEFL are argumentative essays , and good essays should use facts and evidence to support their claim ( Wingate , 2012 ) . We observe that texts assigned a low score frequently include claims without convincing evidence . This causes our model to capture focus based on pronouns more frequently in these texts . In contrast , texts assigned a high score include convincing evidence to support claims , and this lets our model capture different types of foci in these texts .
Error Analysis
Finally , we conduct an error analysis to investigate how our model works differently compared to previous coherence models on TOEFL . We first compare the predicted scores with Moon et al . ( 2019 ) and a simple model which only considers context , averaged - XLNet . These two baselines show biased predictions in the middle score . We suspect that this is caused by the label bias in TOEFL ( Blanchard et al . , 2013 ) . Biased label distributions cause biased predictions , and they benefit from these biased predictions . In contrast , our model benefits more from predicting high scores correctly as well as other scores , indicating that our coherence model assesses text quality better . We then compare with the previous state of the art ( Jeon and Strube , 2020 ) . This baseline induces discourse structure to model structural coherence . It captures semantic relations between discourse segments , not just between adjacent sentences . We observe two error cases when this baseline struggles to predict correctly . It predicts scores lower than the ground - truth score for texts which lack support and evidence for claims . However , these texts have a well - organized paragraph for one or two claims . We suspect that this leads human annotators to assign a mid or a high score though the text is not well - organized overall . In contrast , it predicts scores higher than ground - truth scores when unrelated claims are listed or claims are listed  
Conclusions
We propose a neural coherence model based on entities by constraining the input to noun phrases . This makes our model better explainable and sets a new state of the art in end applications . It also allows us to reveal that texts of higher quality are neither semantically too consistent nor too variant .
Our findings suggest a few interesting directions for future work . Our analysis shows that pretrained language models frequently exploit coreference relations to capture semantic relations . We could design an advanced neural model which exploits these relations explicitly . Lastly , our work could be extended to a multilingual setup . Our model is not tied to a specific pretrained language model but connect a language model with linguistic insights . It can employ a multilingual model ( Xue et al . , 2021 ) , and our datasets can be translated to other languages .
A Training and Parameters
For the three datasets , we use a mini - batch size of 32 with random - shuffle . The ADAM optimizer is used to train our models with a learning rate of 0.001 and epsilon of 1e-4 . We evaluate performance for 25 epochs . For the baseline models which do not use a pretrained language model , we use Glove pretrained embeddings with 100dimensional for TOEFL and with 50 - dimensional for NYT . We clip gradients by 1.0 . To update sentence representations obtained by a pretrained language model , we use the same dimension of the pretrained language model on a tree - transformer . We manually tune hyperparameters .
We encode adjacent two sentences at once using XLNet instead of the whole document at once . Our dataset consists of long documents i.e. , journal articles with more than 3,000 tokens . For employing the pretrained model , it is practically infeasible to encode all words in a document at once due to memory limitations . We use 23 GB GPU memory a NVidia P40 on ADC and AES and 46 GB GPU memory of two NVidia P40s for each run on AWQ . For training our model , it takes approximately 0.8 days on TOEFL , 6.5 days on NYT , and 0.6 days on GCDC .
B Data Description Details
Table 9 describes statistics on two datasets , TOEFL 5 and NYT 6 . We split a text at the sentence level by Stanford Stanza library , and tokenize them by the XLNet tokenizer . Table 10 describes the topic of each prompt in TOEFL . They are all openended tasks , that do not have given context but require students to submit their opinion .
C Focus Examples
Table 11 shows the cases that the pretrained language model , XLNet , captures the undesirable ( sub-)words as focus . We observe that the subword tokenizer often split named entities into subword tokens unexpectedly , and some words are unexpectedly split into subword tokens as prefixes and suffixes , such as " _ un " or " ition " . These observations suggest that we need to consider tokens as a span to capture the meaning of words better .  
D Evaluations Details
We report not only the more details of the performance on test sets ( Table 12 ) but also the performance on validation sets on the AES task ( Table 13 ) .
E Analysis Details
We compare the focus captured on ( sub-)words and the focus constrained to entities on more datasets ( Table 14 ) . We observe that our entity modeling leads to better explainability .   Example Sentence 3 Einstein 's defection from the quantum revolution was a blow to his more conservative colleagues .
Focus candidates captured by XLNet " _ stein " , " _ was " , " _ his " , " _ more " , " _ blow " , " _ the " , " " ' , " ein " , ... , " _ from " Example Sentence 4 On Thursday , responding to evidence that Celebrex and Bextra may pose the same risks , the F.D.A. recommended that physicians limit their use of the drugs .
Focus candidates captured by XLNet " _ on " , " _ th " , " _ ur " , " _ s " , " _ day " , " , " , " _ responding " , " _ to " , " _ evidence " , ... , " _ drugs " Example Sentence 5 Dr. Elizabeth Tindall , president of the American College of Rheumatology , said in a statement last week .
Focus candidates captured by XLNet " _ said " , " ology " , " _ week " , " _ in " , " _ of " , " , " , " _ college " , " _ the " , " _ last " , " ll " , " _ a " , ... , " dr " Example Sentence 6 Current American testing focuses only on finding the prion that causes bovine spongiform encephalopa thy in cows and " variant " Creutzfeldt - Jakob disease in humans .
Focus candidates captured by XLNet " opathy " , " t " , " _ disease " , " - " , " _ humans " , " _ the " , " vine " , " en " , " _ and " , " _ cre " , ... , " _ pr " Example Sentence 7 These days the concepts of family values , traditions and culture have lost their meaning and the you$g people often end up neglecting these important concepts .
Focus candidates captured by XLNet " _ concepts " , " ing " , " _ lost " , " _ of " , " _ concepts " , " _ the " , " _ values " , " _ these " , " _ end " , " people " , " _ have " , ... , " radi " Example Sentence 8 The community plays an important role in shaping a personhis desires , actions , thoughts , opinions etc . Focus candidates captured by XLNet " _ etc " , " _ role " , " s " , " _ desires " , " s " , " _ person " , " _ the " , " op " , " ions ' , ... , " _ important " Example Sentence 9 On the other hand , the time that have been used by them to community service is enough already for the fact that learning is the primary task that they should focus on at their age anyway .
Focus candidates captured by XLNet " _ on " , " _ them " , " _ at " , " _ on " , " _ already " , " _ to " , " _ used " , " _ they " , " _ for " , " _ that " , " _ anyway " , " _ by " , ... , " _ the "      My talant 6 is to study a law . 7
When I 6 was first grade in the highschool , I 7 had a friend who called Che - Jea - Heong . 8
He was very special friend 7,8 . 9
He always tried to think strange way 8,9 . 10 At first , I 9 did n't want to talk with him , but when we 10 talked about the talant , we became a friend . 11 Actually , his father 10,11 is police . 12 And his family 11 is very poor . 13 So , first we 12 started to talk his father . 14 why he 12,13 is poor . 15 After that we 13,14    However my friend , who seriously wanted to become a doctor , took all science courses 8,9 , because she wanted to explore her passion . 9
As a result , I believe it would be better to have a broad knowledge of many subjects 9,10 before specializing one , unless you have found something that you really want to pursue . 10 Moreover , by studying more subjects 10 , it 11 makes people easy to dive in conversations with new people . 11 Everyone have different backgrounds , therefore if you have knowledge from different areas 12 , it 11 could be easier to socialize with people whom have different fields from we have . 12 A way of knowing more subjects 12,13 can be to read every section of the newspaper such as Businss , World , Entertainment etc . 13 This could help us to know more knowledge 13 and therefore we can be more talkative meeting new people 14 .
Error Type Example Essay C 1 In my opinion is better to have a knowledge specialize in one particular subject since this is better to know a thing as well as you can . This is true in all the experiences of the life : refered to the university , e.g. , the italian university , we can take the example of the of the two years of specialization . An other example we can see in a top - tier company , in fact each people that there are in this have a specific work to do and this bring to an excellent final operation . A person that are magnifically prepared on one thing will arrive at a sicure result because that " " is your bred " " ; we can also observe that the most good professors , scientists , sport players are all specialize on that they work and do not specialize on many works . We can also observe that the colloboration of great brains , each of them specialized on a thing , is important in many ways of the our life . C 2 I strongly agree with the statement that knowing several subjects and being polyvalent in various fields is much more important that specializing in one area . These days , things are changing so fast that the moment you start a career or a specialization , the minute the facts and figures of the subject have changed . This essence of broad knowledge is what makes people succeed in the world . Unless you are 100 % sure that you vocationally desire to specialize in a subject , the risk of not finding a suitable job because of the deviation of job offering is too high . Both with respect to time and money . For example , imagine that you decide to study IT sometime around the Internet boom . After you finish the 5 years of studying , you get out to society with high hopes and great expectations and suddenly you realize that the world does not need for IT people anymore because the market crashed down ! Then you would most probably regret not to have chosen a more general Engineering degree such as an Electronical Engineering degree . Take the example of a devoted music students that really loves to play music to the point that they drop classes so they can go and play their music . Perhaps , they will become a succesful singer or solo player , but the chances that they fail are there and when that really comes true , they will not be able to attend university classes because they did n't passed high - school . Good and innovative ideas often are the result of composing other ideas . If on one side , you know how pollution of carbon dioxide is chemically produced and on the other , you are an expert on plant species , perhaps you can find a way to create a system to purify the air in the world . And moreover , if you have skills of marchandising and marketing , you can probably be in the Forbes ' next month main page . Think that you can always specialize in the future . Going from the trunck of a tree to the tip of a branch is easy , but getting from one tip to another tip is , literally , as going back in time .   Error Type Example Essay C 3 It seems difficult to choose one direction , becuse they are also have colorful life between the young people and the older people , but it does not mean , they are similar to me . I would like to agree with the young people enjoy life more than older people do , if a personal quality can be considerated as criterion to choose things . First of all , nowadays , era of information , many young people enjoy their life via the internet , even everything is possible in the digital industy . For instance , if a grandson of the older people live abroad , and the communication between the grandson and the grandfather is only via the telephone instead the internet online chatting what is cheaper than the international telephone call , but the older people can not use the internet , even they can not use a computer .
On the other hand , the young people can adapt an environment quickly , so that they can migrate to another city for the different experience . most of older persons can not accept the different enviroment and what they will eat in the different areas , if the older person migrate to other citys or countries , they will be illness easier .
The important things determining the young people enjoy life better is that they are educated in the signifcant era of information , so they are developed with the world development . For all mentioned above is why I agree with the statement that young people enjoy life more than older people do . Now , I do strongly agree with the statement .
C 4
Yes , it is better to have a broad knowledge of many academic subjects than specialisze in one specific area because of various reasons .
If people have knowledge about a particular subject , it is good . But if they want to refrain themselves from foraying from other subjects they should make sure that they are very thorough with that subject . Because finally they should find a job on that basis only and more ver all the academic topics are interconnected so , it imperative to have knowledgein various fields .
The above option would be good only if they find a job . They should always keep in mind the different possibilities in their carreer . They should ask themselves " " what if i do nt get a job in my desired field of study ? " " For instance I am a mechanical engineering student . as every one knows there is a difficult of getting jobs for mechanical engineers.if i continue with the same field would be left unemployed . Here I need to have an alternate option . I have my alternate option as computer sciences .I started learning some computer subjects . Now even if i do not get a job in my field of study , i may have a chance of getting it in field of computers . This would not leave me unemployed . I personally feel that being employed is better than being unemployed . This criteria not only works for two fields of same backround , it also works for a technical background and an arts background . For example , an electrical engineer who does not have a job and whose hobby is singing , can survive by giving some stage shows . Which would also be considered as an employment . Additionally , broader knowlege would not leave you speechless when you are in a group . Because when a group is discussing a topic and if you are silent , you may feel embarassing with that . But if you are familiar with the topic you can also give your opinion on the topic . this is possible only if you do not confine yourself to a particular field . Therefore , I conclude that having a broad knowledge is better than to specialize in one subject .  
Acknowledgments
The authors would like to thank the anonymous reviewers for their comments . This work has been funded by the Klaus Tschira Foundation , Heidelberg , Germany . The first author has been supported by a Heidelberg Institute for Theoretical Studies Ph.D. scholarship .
