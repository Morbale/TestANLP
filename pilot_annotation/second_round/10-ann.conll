-DOCSTART- -X- O
Modeling -X- _ O
Bilingual -X- _ O
Conversational -X- _ O
Characteristics -X- _ O
for -X- _ O
Neural -X- _ B-TaskName
Chat -X- _ I-TaskName
Translation -X- _ I-TaskName

Neural -X- _ B-TaskName
chat -X- _ I-TaskName
translation -X- _ I-TaskName
aims -X- _ O
to -X- _ O
translate -X- _ O
bilingual -X- _ O
conversational -X- _ O
text -X- _ O
, -X- _ O
which -X- _ O
has -X- _ O
a -X- _ O
broad -X- _ O
application -X- _ O
in -X- _ O
international -X- _ O
exchanges -X- _ O
and -X- _ O
cooperation -X- _ O
. -X- _ O
Despite -X- _ O
the -X- _ O
impressive -X- _ O
performance -X- _ O
of -X- _ O
sentence -X- _ O
- -X- _ O
level -X- _ O
and -X- _ O
context -X- _ O
- -X- _ O
aware -X- _ O
Neural -X- _ B-TaskName
Machine -X- _ I-TaskName
Translation -X- _ I-TaskName
( -X- _ O
NMT -X- _ B-TaskName
) -X- _ O
, -X- _ O
there -X- _ O
still -X- _ O
remain -X- _ O
challenges -X- _ O
to -X- _ O
translate -X- _ O
bilingual -X- _ O
conversational -X- _ O
text -X- _ O
due -X- _ O
to -X- _ O
its -X- _ O
inherent -X- _ O
characteristics -X- _ O
such -X- _ O
as -X- _ O
role -X- _ O
preference -X- _ O
, -X- _ O
dialogue -X- _ O
coherence -X- _ O
, -X- _ O
and -X- _ O
translation -X- _ O
consistency -X- _ O
. -X- _ O
In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
aim -X- _ O
to -X- _ O
promote -X- _ O
the -X- _ O
translation -X- _ O
quality -X- _ O
of -X- _ O
conversational -X- _ O
text -X- _ O
by -X- _ O
modeling -X- _ O
the -X- _ O
above -X- _ O
properties -X- _ O
. -X- _ O
Specifically -X- _ O
, -X- _ O
we -X- _ O
design -X- _ O
three -X- _ O
latent -X- _ O
variational -X- _ O
modules -X- _ O
to -X- _ O
learn -X- _ O
the -X- _ O
distributions -X- _ O
of -X- _ O
bilingual -X- _ O
conversational -X- _ O
characteristics -X- _ O
. -X- _ O
Through -X- _ O
sampling -X- _ O
from -X- _ O
these -X- _ O
learned -X- _ O
distributions -X- _ O
, -X- _ O
the -X- _ O
latent -X- _ O
variables -X- _ O
, -X- _ O
tailored -X- _ O
for -X- _ O
role -X- _ O
preference -X- _ O
, -X- _ O
dialogue -X- _ O
coherence -X- _ O
, -X- _ O
and -X- _ O
translation -X- _ O
consistency -X- _ O
, -X- _ O
are -X- _ O
incorporated -X- _ O
into -X- _ O
the -X- _ O
NMT -X- _ B-TaskName
model -X- _ O
for -X- _ O
better -X- _ O
translation -X- _ O
. -X- _ O
We -X- _ O
evaluate -X- _ O
our -X- _ O
approach -X- _ O
on -X- _ O
the -X- _ O
benchmark -X- _ O
dataset -X- _ O
BConTrasT -X- _ B-DatasetName
( -X- _ O
English⇔German -X- _ O
) -X- _ O
and -X- _ O
a -X- _ O
self -X- _ O
- -X- _ O
collected -X- _ O
bilingual -X- _ O
dialogue -X- _ O
corpus -X- _ O
, -X- _ O
named -X- _ O
BMELD -X- _ B-DatasetName
( -X- _ O
English⇔Chinese -X- _ O
) -X- _ O
. -X- _ O
Extensive -X- _ O
experiments -X- _ O
show -X- _ O
that -X- _ O
our -X- _ O
approach -X- _ O
notably -X- _ O
boosts -X- _ O
the -X- _ O
performance -X- _ O
over -X- _ O
strong -X- _ O
baselines -X- _ O
by -X- _ O
a -X- _ O
large -X- _ O
margin -X- _ O
and -X- _ O
significantly -X- _ O
surpasses -X- _ O
some -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
context -X- _ O
- -X- _ O
aware -X- _ O
NMT -X- _ B-TaskName
models -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
BLEU -X- _ B-MetricName
and -X- _ O
TER -X- _ B-MetricName
. -X- _ O
Additionally -X- _ O
, -X- _ O
we -X- _ O
make -X- _ O
the -X- _ O
BMELD -X- _ B-DatasetName
dataset -X- _ O
publicly -X- _ O
available -X- _ O
for -X- _ O
the -X- _ O
research -X- _ O
community -X- _ O
. -X- _ O
1 -X- _ O

A -X- _ O
conversation -X- _ O
may -X- _ O
involve -X- _ O
participants -X- _ O
that -X- _ O
speak -X- _ O
in -X- _ O
different -X- _ O
languages -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
one -X- _ O
speaking -X- _ O
in -X- _ O
English -X- _ O
and -X- _ O
another -X- _ O
in -X- _ O
Chinese -X- _ O
) -X- _ O
. -X- _ O
Fig -X- _ O
. -X- _ O
1 -X- _ O
shows -X- _ O
an -X- _ O
example -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
English -X- _ O
role -X- _ O
R -X- _ O
1 -X- _ O
and -X- _ O
the -X- _ O
Chinese -X- _ O
role -X- _ O
R -X- _ O
2 -X- _ O
are -X- _ O
talking -X- _ O
about -X- _ O
the -X- _ O
" -X- _ O
boat -X- _ O
" -X- _ O
. -X- _ O
The -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
Chinese -X- _ O
utterances -X- _ O
are -X- _ O
presented -X- _ O
in -X- _ O
pinyin -X- _ O
style -X- _ O
. -X- _ O
R -X- _ O
i -X- _ O
: -X- _ O
Role -X- _ O
i. -X- _ O
The -X- _ O
dashed -X- _ O
arrows -X- _ O
mark -X- _ O
the -X- _ O
translation -X- _ O
direction -X- _ O
. -X- _ O
The -X- _ O
green -X- _ O
and -X- _ O
red -X- _ O
arrows -X- _ O
represent -X- _ O
the -X- _ O
monolingual -X- _ O
and -X- _ O
bilingual -X- _ O
conversation -X- _ O
flow -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O
Although -X- _ O
the -X- _ O
translation -X- _ O
of -X- _ O
Y -X- _ O
5 -X- _ O
produced -X- _ O
by -X- _ O
the -X- _ O
" -X- _ O
S -X- _ B-TaskName
- -X- _ I-TaskName
NMT -X- _ I-TaskName
" -X- _ O
( -X- _ O
a -X- _ O
context -X- _ O
- -X- _ O
free -X- _ O
sentencelevel -X- _ O
NMT -X- _ O
system -X- _ O
) -X- _ O
is -X- _ O
reasonable -X- _ O
at -X- _ O
the -X- _ O
sentence -X- _ O
level -X- _ O
, -X- _ O
the -X- _ O
coherence -X- _ O
of -X- _ O
the -X- _ O
entire -X- _ O
dialogue -X- _ O
translation -X- _ O
is -X- _ O
poor -X- _ O
. -X- _ O

Although -X- _ O
sentence -X- _ B-TaskName
- -X- _ I-TaskName
level -X- _ I-TaskName
Neural -X- _ I-TaskName
Machine -X- _ I-TaskName
Translation -X- _ I-TaskName
( -X- _ O
NMT -X- _ B-TaskName
) -X- _ O
( -X- _ O
Sutskever -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
; -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
; -X- _ O
Hassan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Yan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
has -X- _ O
achieved -X- _ O
promising -X- _ O
progress -X- _ O
, -X- _ O
it -X- _ O
still -X- _ O
faces -X- _ O
challenges -X- _ O
in -X- _ O
accurately -X- _ O
translating -X- _ O
conversational -X- _ O
text -X- _ O
due -X- _ O
to -X- _ O
abandoning -X- _ O
the -X- _ O
dialogue -X- _ O
history -X- _ O
, -X- _ O
which -X- _ O
leads -X- _ O
to -X- _ O
role -X- _ O
- -X- _ O
irrelevant -X- _ O
, -X- _ O
incoherent -X- _ O
and -X- _ O
inconsistent -X- _ O
translations -X- _ O
( -X- _ O
Mirkin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
; -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017a -X- _ O
; -X- _ O
Läubli -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Toral -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O
Further -X- _ O
, -X- _ O
context -X- _ O
- -X- _ O
aware -X- _ O
NMT -X- _ O
( -X- _ O
Tiedemann -X- _ O
and -X- _ O
Scherrer -X- _ O
, -X- _ O
2017 -X- _ O
; -X- _ O
Voita -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018Voita -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
, -X- _ O
2019aMaruf -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Ma -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
can -X- _ O
be -X- _ O
directly -X- _ O
applied -X- _ O
to -X- _ O
chat -X- _ O
translation -X- _ O
through -X- _ O
incorporating -X- _ O
the -X- _ O
dialogue -X- _ O
history -X- _ O
but -X- _ O
can -X- _ O
not -X- _ O
obtain -X- _ O
satisfactory -X- _ O
results -X- _ O
in -X- _ O
this -X- _ O
sce -X- _ O
- -X- _ O
nario -X- _ O
( -X- _ O
Moghe -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
One -X- _ O
important -X- _ O
reason -X- _ O
is -X- _ O
the -X- _ O
lack -X- _ O
of -X- _ O
explicitly -X- _ O
modeling -X- _ O
the -X- _ O
inherent -X- _ O
bilingual -X- _ O
conversational -X- _ O
characteristics -X- _ O
, -X- _ O
e.g. -X- _ O
, -X- _ O
role -X- _ O
preference -X- _ O
, -X- _ O
dialogue -X- _ O
coherence -X- _ O
, -X- _ O
and -X- _ O
translation -X- _ O
consistency -X- _ O
, -X- _ O
as -X- _ O
pointed -X- _ O
out -X- _ O
by -X- _ O
Farajian -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
aim -X- _ O
to -X- _ O
learn -X- _ O
a -X- _ O
model -X- _ O
that -X- _ O
can -X- _ O
capture -X- _ O
inherent -X- _ O
characteristics -X- _ O
in -X- _ O
the -X- _ O
bilingual -X- _ O
dialogue -X- _ O
history -X- _ O
for -X- _ O
producing -X- _ O
high -X- _ O
- -X- _ O
quality -X- _ O
translations -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
using -X- _ O
the -X- _ O
context -X- _ O
for -X- _ O
better -X- _ O
translations -X- _ O
( -X- _ O
Farajian -X- _ O

All -X- _ O
our -X- _ O
Transformer -X- _ O
models -X- _ O
contain -X- _ O
N -X- _ O
e -X- _ O
= -X- _ O
6 -X- _ O
encoder -X- _ O
layers -X- _ O
and -X- _ O
N -X- _ O
d -X- _ O
= -X- _ O
6 -X- _ O
decoder -X- _ O
layers -X- _ O
and -X- _ O
all -X- _ O
models -X- _ O
are -X- _ O
trained -X- _ O
using -X- _ O
THUMT -X- _ O
( -X- _ O
Tan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
framework -X- _ O
. -X- _ O
We -X- _ O
conduct -X- _ O
experiments -X- _ O
on -X- _ O
the -X- _ O
validation -X- _ O
set -X- _ O
of -X- _ O
En⇒De -X- _ O
to -X- _ O
select -X- _ O
the -X- _ O
hyperparameters -X- _ O
of -X- _ O
context -X- _ O
length -X- _ O
and -X- _ O
latent -X- _ O
dimension -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
then -X- _ O
shared -X- _ O
for -X- _ O
all -X- _ O
tasks -X- _ O
. -X- _ O
For -X- _ O
the -X- _ O
results -X- _ O
and -X- _ O
more -X- _ O
details -X- _ O
( -X- _ O
other -X- _ O
hyperparameters -X- _ O
setting -X- _ O
and -X- _ O
average -X- _ O
running -X- _ O
time -X- _ O
) -X- _ O
, -X- _ O
please -X- _ O
refer -X- _ O
to -X- _ O
Appendix -X- _ O
B -X- _ O
, -X- _ O
C -X- _ O
, -X- _ O
and -X- _ O
D -X- _ O
. -X- _ O

Role -X- _ O
Preference -X- _ O
and -X- _ O
Dialogue -X- _ O
Coherence -X- _ O
. -X- _ O
As -X- _ O
shown -X- _ O
in -X- _ O
Fig -X- _ O
. -X- _ O
6 -X- _ O
, -X- _ O
we -X- _ O
observe -X- _ O
that -X- _ O
the -X- _ O
baseline -X- _ O
models -X- _ O
and -X- _ O
the -X- _ O
context -X- _ O
- -X- _ O
aware -X- _ O
models -X- _ O
except -X- _ O
" -X- _ O
V -X- _ O
- -X- _ O
Transformer+FT -X- _ B-MethodName
" -X- _ O
can -X- _ O
not -X- _ O
preserve -X- _ O
the -X- _ O
role -X- _ O
preference -X- _ O
information -X- _ O
, -X- _ O
e.g. -X- _ O
, -X- _ O
joy -X- _ O
emotion -X- _ O
, -X- _ O
even -X- _ O
these -X- _ O
" -X- _ O
* -X- _ O
-Transformer+FT -X- _ B-MethodName
" -X- _ O
models -X- _ O
incorporate -X- _ O
the -X- _ O
bilingual -X- _ O
conversational -X- _ O
history -X- _ O
into -X- _ O
the -X- _ O
encoder -X- _ O
. -X- _ O
The -X- _ O
" -X- _ O
V -X- _ O
- -X- _ O
Transformer+FT -X- _ B-MethodName
" -X- _ O
model -X- _ O
produces -X- _ O
very -X- _ O
slightly -X- _ O
emotional -X- _ O
elements -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
" -X- _ O
zěnme -X- _ O
? -X- _ O
" -X- _ O
) -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
latent -X- _ O
variable -X- _ O
over -X- _ O
the -X- _ O
source -X- _ O
sentence -X- _ O
capturing -X- _ O
relevant -X- _ O
preference -X- _ O
information -X- _ O
. -X- _ O
Meanwhile -X- _ O
, -X- _ O
we -X- _ O
find -X- _ O
that -X- _ O
all -X- _ O
comparison -X- _ O
models -X- _ O
can -X- _ O
not -X- _ O
generate -X- _ O
a -X- _ O
coherent -X- _ O
translation -X- _ O
. -X- _ O
The -X- _ O
reason -X- _ O
may -X- _ O
be -X- _ O
that -X- _ O
they -X- _ O
fail -X- _ O
to -X- _ O
capture -X- _ O
the -X- _ O
conversation -X- _ O
- -X- _ O
level -X- _ O
coherence -X- _ O
clue -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
" -X- _ O
boat -X- _ O
" -X- _ O
. -X- _ O
By -X- _ O
contrast -X- _ O
, -X- _ O
we -X- _ O
explicitly -X- _ O
model -X- _ O
the -X- _ O
two -X- _ O
characteristics -X- _ O
through -X- _ O
tailored -X- _ O
latent -X- _ O
variables -X- _ O
and -X- _ O
thus -X- _ O
obtain -X- _ O
satisfactory -X- _ O
results -X- _ O
. -X- _ O

