{"title": "Text Adversarial Purification as Defense against Adversarial Attacks", "authors": "Linyang Li; Demin Song; Xipeng Qiu", "pub_date": "", "abstract": "Adversarial purification is a successful defense mechanism against adversarial attacks without requiring knowledge of the form of the incoming attack. Generally, adversarial purification aims to remove the adversarial perturbations therefore can make correct predictions based on the recovered clean samples. Despite the success of adversarial purification in the computer vision field that incorporates generative models such as energy-based models and diffusion models, using purification as a defense strategy against textual adversarial attacks is rarely explored. In this work, we introduce a novel adversarial purification method that focuses on defending against textual adversarial attacks. With the help of language models, we can inject noise by masking input texts and reconstructing the masked texts based on the masked language models. In this way, we construct an adversarial purification process for textual models against the most widely used word-substitution adversarial attacks. We test our proposed adversarial purification method on several strong adversarial attack methods including Textfooler and BERT-Attack and experimental results indicate that the purification algorithm can successfully defend against strong word-substitution attacks.", "sections": [{"heading": "Introduction", "text": ["Adversarial examples (Goodfellow et al., 2014) can successfully mislead strong neural models in both computer vision tasks (Carlini and Wagner, 2016) and language understanding tasks (Alzantot et al., 2018;Jin et al., 2019). An adversarial example is a maliciously crafted example attached with an imperceptible perturbation and can mislead neural networks. To defend attack examples of images, the most effective method is adversarial training (Goodfellow et al., 2014;Madry et al., 2019) which is a mini-max game used to incorporate perturbations into the training process.", "Defending adversarial attacks is extremely important in improving model robustness. However, defending adversarial examples in natural languages is more challenging due to the discrete nature of texts. That is, gradients cannot be used directly in crafting perturbations. The substitutionbased adversarial examples are more complicated than gradient-based adversarial examples in images, making it difficult for neural networks to defend against these substitution-based attacks.", "The first challenge of defending against adversarial attacks in NLP is that due to the discrete nature, these substitution-based adversarial examples can have substitutes in any token of the sentence and each substitute has a large candidate list. This would cause a combinatorial explosion problem, making it hard to apply adversarial training methods. Strong attacking methods such as Jin et al. (2019) show that using the crafted adversarial examples as data augmentation in adversarial training cannot effectively defend against these substitutionbased attacks. Further, defending strategies such as adversarial training rely on the assumption that the candidate lists of the substitutions are accessible. However, the candidate lists of the substitutions should not be exposed to the target model; that is, the target model should be unfamiliar to the candidate list of the adversarial examples. In real-world defense systems, the defender is not aware of the strategy the potential attacks might use, so the assumption that the candidate list is available would significantly constrain the potential applications of these defending methods.", "Considering that it is challenging to defend against textual adversarial attacks when the form of the attacks cannot be acknowledged in advance, we introduce a novel adversarial purification method as a feasible defense mechanism against these attacks. The adversarial purification method is to purify adversarially perturbed input samples before making predictions (Srinivasan et al., 2021;Shi et al., 2021;Yoon et al., 2021). The major works about adversarial purification focus on purifying continuous inputs such as images, therefore these works explore different generative models such as GANs (Samangouei et al., 2018), energy-based models (EBMs) (LeCun et al., 2006) and recently developed diffusion models (Song et al., 2021;Nie et al., 2022). However, in textual adversarial attacks, the inputs are discrete tokens which makes it more challenging to deploy previous adversarial purification methods.", "Therefore, we introduce a purification mechanism with the help of masked language models. We first consider the widely used masking process to inject noise into the input; then we recover the clean texts from the noisy inputs with the help of the masked language models (e.g. a BERT (Devlin et al., 2018)). Further, considering that the iterative process in previous adversarial purification algorithms can be extremely costly (e.g. a VP-SDE process in diffusion models (Song et al., 2021)), we instead simplify the iterative process to an ensemble-purifying process that conducting adversarial purification multiple times to obtain an ensembled result as a compromise to the time cost in traditional adversarial purification process.", "Through extensive experiments, we prove that the proposed text adversarial purification algorithm can successfully serve as defense against strong attacks such as Textfooler and BERT-Attack. Experiment results show that the accuracy under attack in baseline defense methods is lower than random guesses, while after text purification, the performance can reach only a few percent lower than the original accuracy when the candidate range of the attack is limited. Further, extensive results indicate that the candidate range of the attacker score is essential for successful attacks, which is a key factor in maintaining the semantics of the adversaries. Therefore we also recommend that future attacking methods can focus on achieving successful attacks with tighter constraints.", "To summarize our contributions:", "(1) We raise the concern of defending substitution-based adversarial attacks without acknowledging the form of the attacks in NLP tasks.", "(2) To the best of our knowledge, we are the first to consider adversarial purification as a defense against textual adversarial attacks exemplified by strong word-substitution attacks and combine text adversarial purification with pre-trained models.", "(3) We perform extensive experiments to demonstrate that the adversarial purification method is capable of defending strong adversarial attacks, which brings a new perspective to defending textual adversarial attacks."], "n_publication_ref": 16, "n_figure_ref": 0}, {"heading": "Related Work", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Adversarial Attacks in NLP", "text": ["In NLP tasks, current methods use substitutionbased strategies (Alzantot et al., 2018;Jin et al., 2019;Ren et al., 2019) to craft adversarial examples. Most works focus on the score-based blackbox attack, that is, attacking methods know the logits of the output prediction. These methods use different strategies (Yoo et al., 2020;Morris et al., 2020b) to find words to replace, such as genetic algorithm (Alzantot et al., 2018), greedy-search (Jin et al., 2019;Li et al., 2020) or gradient-based methods (Ebrahimi et al., 2017; and get substitutes using synonyms (Jin et al., 2019;Mrk\u0161i\u0107 et al., 2016;Ren et al., 2019) or language models (Li et al., 2020;Garg and Ramakrishnan, 2020;Shi et al., 2019)."], "n_publication_ref": 15, "n_figure_ref": 0}, {"heading": "Adversarial Defenses", "text": ["We divide the defense methods for wordsubstitution attacks by whether the defense method requires knowledge of the form of the attack.", "When the candidate list is known, recent works introduce defense strategies that incorporate the candidates of the words to be replaced as an augmentation. Jin et al. (2019); Li et al. (2020); Si et al. (2020) uses generated adversaries to augment the classifier for better defense performances; Jia et al. (2019);  introduce a certified robust model to construct a certified space within the range of a candidate list therefore the substitutions in the candidate list cannot perturb the model. Zhou et al. (2020); Dong et al. (2021) construct a convex hull based on the candidate list which can resist substitutions in the candidate list.", "To defend unknown attacks, NLP models can incorporate gradient-based adversarial training strategies (Miyato et al., 2016;Madry et al., 2019) since recent works (Ebrahimi et al., 2017;Zhu et al., 2019;Li and Qiu, 2020) show that gradient-based adversarial training can also improve defense performances against wordsubstitution attacks.  Compared with Image Purification, we use masked language models to recover noisy texts to purify adversarial texts as a defense against word-substitutions attacks."], "n_publication_ref": 11, "n_figure_ref": 0}, {"heading": "Adversarial Purification", "text": ["Adversarial purification is a defense strategy that uses generative models to purify adversarial inputs before making predictions, which is a promising direction in adversarial defense. Samangouei et al. (2018) uses a defensive GAN framework to build clean images to avoid adversarial attacks. Energybased models (EBMs) are used to purify attacked images via Langevin dynamics (LeCun et al., 2006). Score-based models (Yoo et al., 2020) is also introduced as a purification strategy. Recent works focus on exploring diffusion models as the purification model in purifying the attacked images (Nie et al., 2022). Though widely explored, adversarial purification strategy is less explored in the NLP field.", "3 Text Adversarial Purification"], "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Background of Adversarial Purification", "text": ["A classic adversarial purification process is to gradually purify the input through T steps of purification runs. As seen in Figure 1, the purification process in the image domain is to first construct an input x \u2032 from the perturbed input x by injecting random noise. Then the purification algorithm will recover the clean image x from the noisy image x \u2032 which usually takes multiple rounds. The intuition of such a purification process is that the recovered inputs will not contain adversarial effects.", "Specifically, in the score-based adversarial purification (Yoo et al., 2020), the sample injected with random noise is x \u2032 = x + \u03b5 where \u03b5 \u223c N (0, \u03c3 2 I) and the goal is to purify x \u2032 with score network s \u03b8 . In a continuous time step where x 0 = x \u2032 , the goal is to recover x 0 through a score-based generative model", "x t = x t\u22121 + \u03b1 t\u22121 s \u03b8 (x t\u22121 )", "where \u03b1 is the step size related to x t\u22121 . After T times of generation, the recovered x = x T is used in the final prediction which contains less adversarial effect.", "As for the diffusion-based purification methods (Nie et al., 2022), the process includes a forward diffusion process and a reverse recovery process. The noise injection process is a forward stochastic differential equation (SDE), that is, the noisy input x \u2032 = x(T ) and initial perturbed input x = x(0). The diffusion process is x(T ) = \u03b1(T )x(0) + 1 \u2212 \u03b1(T )\u03b5 where \u03b1 is a hyper-parameter and \u03b5 \u223c N (0, \u03c3 2 I). The final purified input x = x(0) where x(0) is the reverse-time SDE generated input from the diffused input x(T )."], "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Text Adversarial Purification with BERT", "text": ["Instead of the iterative purification process used in purifying images, we introduce a novel purification method that purifies the input texts via masking and masks prediction with pre-trained masked language models exemplified by BERT (Devlin et al., 2018). As seen in Figure 1, instead of gradually adding noise and recovering the clean sample from the noisy samples, we inject random noise into the input texts multiple times and recover the noisy data to a clean text based on the mask-prediction ability of the masked language model F m (\u2022).", "Considering that the perturbed text is X, we can inject noise to construct multiple copies", "X \u2032 i = [w 0 , \u2022 \u2022 \u2022 , [MASK], w n , \u2022 \u2022 \u2022 , ].", "We use two simple masking strategies: (1) Randomly mask the input texts; (2) Randomly insert masks into the input texts. Such a random masking process is similar to adding a random noise \u03b5 \u223c N (0, \u03c3 2 I) to the inputs x.", "After constructing multiple noisy inputs, we run the denoise process via masked language models:", "X i = F m (X \u2032 i ).", "With N recovered texts, we are able to make predictions with the classifier F c (\u2022):", "S i = 1 N N i=0 Sof tmax(F c ( X i )) .", "Unlike continuous perturbations to images, word-substitution adversarial samples only contain several perturbed words. Therefore, we consider using a multiple-time mask-and-recover process as text adversarial purification, which makes full use of the pre-trained ability of the masked language models. Compared with the generation process used in image adversarial purification, masked language model-based purification method is easier to implement and utilize in pre-trained modelbased applications as a defense against strong wordsubstitution adversarial attacks."], "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "Combining with Classifier", "text": ["Normal adversarial purification methods are plugand-play processes inserted before the classification, however, the masked language model itself is a widely used classification model. That is, the purification model F m (\u2022) and the classification model F c (\u2022) can share the same model. Therefore, instead of using a normal masked language model such as BERT, we train the classifier and the mask-filling ability as multi-tasks. The classification loss is L c = L(F c (X \u2032 ), y, \u03b8)+L(F c (X), y, \u03b8) and the masked language model loss is L mlm = L(F m (X \u2032 ), X, \u03b8). Here, the input X is the clean text used in training the classifier and the X \u2032 is the random masked text. The loss function L(\u2022) is the cross-entropy loss used in both the text classification head and masked language modeling head in the pre-trained models exemplified by BERT.", "In this way, we are utilizing the pre-trained models to their full ability by using both the mask-filling function learned during the pre-training stage as well as the generalization ability to downstream tasks.  Li and Qiu (2020). In the adversarial training process, a gradient-based perturbation \u03b4 is added to the embedding output of the input text X (for simplicity, we still use X and X \u2032 to denote the embedding output in the Algorithm 1). Then the perturbed inputs are added to the training set in the training process. We combine gradient-based adversarial training with the text purification process."], "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Algorithm 1 Adversarial Training", "text": ["Require: Training Sample X, adversarial step T a 1: X \u2032 \u2190 Inject Noise X 2: \u03b4 0 \u2190 1 \u221a D N (0, \u03c3 2 ) // Init Perturb 3: for t = 0, 1, ...T a do 4: g \u03b4 \u2190 \u25bd \u03b4 (L c + L mlm ) // Get Perturbation 5: \u03b4 t \u2190 ||\u03b4|| F <\u03f5 (\u03b4 t + \u03b1 \u2022 g \u03b4 /||g \u03b4 || F ) 6: L noise \u2190 L(F m (X \u2032 + \u03b4 t ), X, \u03b8) 7: X \u2032 \u2190 X \u2032 + \u03b4 t // Update Input 8: g t+1 = g t + \u25bd \u03b8 (L c + L mlm + L noise ) 9: \u03b8 \u2190 \u03b8 \u2212 g T +1 //", "As illustrated in Algorithm 1, for an adversarial training step, we add perturbations to the masked text X \u2032 and run T a times of updates. We calculate gradients based on both classification losses L c and masked language modeling losses L mlm ; further, as seen in line 6, we also calculate the loss that the masked language model will predict the texts from the perturbed text X \u2032 + \u03b4, which enhanced the text recover ability from noisy or adversarial texts."], "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experiments", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Datasets", "text": ["We use two widely used text classification datasets: IMDB 1 (Maas et al., 2011) and AG's News 2 (Zhang et al., 2015) in our experiments. The IMDB dataset is a bi-polar movie review classification task; the AG's News dataset is a four-class news genre classification task. The average length is 220 words in the IMDB dataset, and 40 words in the AG's News dataset. We use the test set following the Textfooler 1k test set in the main result and sample 100 samples for the rest of the experiments since the attacking process is seriously slowed down when the model is defensive."], "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Attack Methods", "text": ["Popular attack methods exemplified by genetic Algorithm (Alzantot et al., 2018), Textfooler (Jin et al., 2019) and BERT-Attack (Li et al., 2020) can successfully mislead strong models of both IMDB and AG's News task with a very small percentage of substitutions. Therefore, we use these strong adversarial attack methods as the attacker to test the effectiveness of our defense method. The hyperparameters used in the attacking algorithm vary in different settings: we choose candidate list size K to be 12, 48, and 50 which are used in the Textfooler and BERT-Attack methods.", "We use the exact same metric used in Textfooler and BERT-Attack that calculates the after-attack accuracy, which is the targeted adversarial evaluation defined by Si et al. (2020). The after-attack accuracy measures the actual defense ability of the system under adversarial attacks."], "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Victim Models and Defense Baselines", "text": ["The victim models are the fine-tuned pre-train models exemplified by BERT and RoBERTa, which we implement based on Huggingface Transformers 3 (Wolf et al., 2020). As discussed above, there are few works concerning adversarial defenses against attacks without knowing the candidates in NLP tasks. Moreover, previous works do not focus on recent strong attack algorithms such as Textfooler (Jin et al., 2019), BERT-involved attacks (Li et al., 2020;Garg and Ramakrishnan, 2020) Therefore, we first list methods that can defend against adversarial attacks without accessing the candidate list as our baselines:", "Adv-Train (Adv-HotFlip): Ebrahimi et al. (2017) introduces the adversarial training method used in defending against substitution-based adversarial attacks in NLP. It uses gradients to find actual adversaries in the embedding space.", "Virtual-Adv-Train (FreeLB): Li and Qiu (2020); Zhu et al. (2019) use virtual adversaries to improve the performances in fine-tuning pretrained models, which can also be used to deal with adversarial attacks without accessing the candidate list. We follow the standard FreeLB training process to re-implement the defense results.", "Further, there are some works that require the candidate list, it is not a fair comparison with defense methods without accessing the candidates, so we list them separately:", "Adv   (Si et al., 2020) 96.7 3.0 -\u2022AMDA (Si et al., 2020) 96.9 17.4 -\u25b2 ASCC (Dong et al., 2021) "], "n_publication_ref": 9, "n_figure_ref": 0}, {"heading": "Implementations", "text": ["We use BERT-BASE and RoBERTa-BASE models based on the Huggingface Transformers 4 . We modify the adversarial training with virtual adversaries based on the implementation of FreeLB, TAVAT, and FreeLB++. The training hyper-parameters we use are different from FreeLB and TAVAT since we aim to find large perturbations to simulate adversaries. We set adversarial learning rate \u03b1 = 1e-1 to and normalization boundary \u03f5 = 2e-1 in all tasks. We set the multiple purification size N = to 16 for all tasks and we will discuss the selection of N in the later section.", "For our text adversarial purification method, we As for implementing adversarial attack methods, we use the TextAttack toolkit while referring the official codes of the corresponding attack methods 5 (Morris et al., 2020a). The similarity thresholds of the word-substitution range are the main factors of the attacking algorithm. We tune the USE (Cer et al., 2018) constraint 0.5 for the AG task and 0.7 for the IMDB task and 0.5 for the cosinesimilarity threshold of the synonyms embedding (Mrk\u0161i\u0107 et al., 2016) which can reproduce the results of the attacking methods reported."], "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Results", "text": ["As seen in Table 1, the proposed Text Adversarial Purification algorithm can successfully defend strong attack methods. The accuracy of our defending method under attack is significantly higher than non-defense models (50% vs 20% in the IMDB dataset). Compared with previous defense methods, our proposed method can achieve higher defense accuracy in both the IMDB task and AG's News task. The Adv-HotFlip and the FreeLB methods  are effective, which indicates that gradient-based adversaries are not very similar to actual substitutions. We can see that Adv-HotFlip and FreeLB methods achieve similar results (around 30% when K = 12) which indicates that gradient-based adversarial training methods have similar defense abilities no matter whether the adversaries are virtual or real since they are both unaware of the attacker's candidate list. Also, the original accuracy (on the clean data) of our method is only a little lower than the baseline methods, which indicates that the purified texts still contain enough information for classification. The RoBERTa model also shows robustness using both original fine-tuned model and our defensive framework, which indicates our purification algorithm can be used in various pretrained language models. Compared with methods that specifically focus on adversarial defense, our proposed method can still surpass the state-of-theart defense system FreeLB++  and RanMASK .", "Further, the candidate size is extremely important in defending against adversarial attacks, when the candidate size is smaller, exemplified by K = 12, our method can achieve very promising results. As pointed out by Morris et al. (2020b), the candidate size should not be too large that the quality of the adversarial examples is largely damaged.", "As seen in Table 2, we compare our method with previous access-candidates defense methods.", "When defending against the widely used Textfooler attack and genetic attack (Alzantot et al., 2018), our method can achieve similar accuracy even compared with known-candidates defense methods. As seen, the data augmentation method cannot significantly improve model robustness since the candidates can be very diversified. Therefore, using generated adversarial samples as an augmentation strategy does not guarantee robustness against greedy-searched methods like Textfooler and BERT-Attack."], "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Analysis", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Ablations", "text": ["As we design an adversarial purification algorithm with masked language models and propose a multiple-recovering strategy, we aim to explore which process helps more in the purification defense system. Plus, we combine classifiers within the purification model so it is also important to explore whether such a combination is helpful.", "For each type of purification method, we test whether the specific purification process we propose is effective. That is, we test whether making multiple recoveries in the purification process is helpful; also, we test whether using both masking tokens and inserting additional masks is helpful.", "As seen in Table 3, we can summarize that:", "(1) Multi-time recovering is necessary: in the image domain, multiple reconstructions with a continuous time purification process are necessary."], "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Texts", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Confidence (Positive)", "text": ["Clean-Sample I have the good common logical sense to know that oil can not last forever and I am acutely aware of how much of my life in the suburbs revolves around petrochemical products. I've been an avid consumer of new technology and I keep running out of space on powerboards -so..."], "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "93.2%", "text": ["Adv. of BERT I possess the good common logical sense to realize that oil can not last forever and I am acutely aware of how much of my life in the suburbs spins around petrochemical products. I've been an avid consumer of new technology and I keep running out of space on powerboards -well..."], "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "38.3%", "text": ["Adv. of Text Pure I know the wonderful general sense to knows that oils can not last endless and I am acutely know of how majority of my lived in the city spins around petrochemical products . I've been an amateur consumers of newly technologies and I kept working out of spaces on powerboards ! well... 80.1%"], "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Purified Texts", "text": ["Well I know the wonderful general sense notion to knows that oils production can not last for endless years and I am acutely know of how the majority of my live in the city spins around the petrochemical production ... I've been an amateur consumers of new technologies and I kept working out of spaces on power skateboards! well ... 80.4% I know the wonderful common sense notion to knows that oils can not last forever and I also acutely know of how majority of my lived in the world and around petrochemical production ...  Similarly, the multi-recovery process is important in obtaining high-quality purification results. We can observe that one-time recovery cannot achieve promising defense performances.", "(2) Combining classifiers is effective: we can observe that when we use trained classifiers and masked language models, the defense performances are better than using fine-tuned classifier and vanilla BERT as a masked language model, indicating that such a combined training process is helpful in obtaining more strong defense systems. Also, with gradient-based adversarial training, the purification process can obtain a further boost, indicating that our proposed text purification algorithm can be used together with previous defense methods as an advanced defense system."], "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Example of Purification Results", "text": ["As seen in Table 4, we construct multiple recoveries and use the averaged score as the final classification result. Such a purification process is effective compared with vanilla fine-tuned BERT.", "We can observe that the adversarial sample that successfully attacked the vanilla BERT model only achieves this by replacing only a few tokens. While with the purification process, the attack algorithm is struggling in finding effective substitutions to achieve a successful attack. Even replacing a large number of tokens that seriously hurt the semantics of the input texts, with the purification process involved, the classifier can still resist the adversarial effect. Further, by observing the purified texts, we can find that the purified texts can make predictions correctly though some substitutes still exist in the purified texts, indicating that making predictions based on purified texts using the combined trained classifier can obtain a promising defense performance. That is, our proposed method, though is not a plug-and-play system, can be used as a general system as a defense against substitution-based attacks."], "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusion and Future Work", "text": ["In this paper, we introduce a textual adversarial purification algorithm as a defense against substitution-based adversarial attacks. We utilize the mask-infill ability of pre-trained models to recover noisy texts and use these purified texts to make predictions. Experiments show that the purification method is effective in defending strong adversarial attacks without acknowledging the substitution range of the attacks. We are the first to consider the adversarial purification method with a multiple-recovering strategy in the text domain while previous successes of adversarial purification strategies usually focus on the image field. Therefore, we hope that the adversarial purification method can be further explored in NLP applications as a powerful defense strategy."], "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Limitations", "text": ["In this paper, we discuss an important topic in the NLP field, the defense against adversarial attacks in NLP applications. We provide a strong defense strategy against the most widely used word substitution attacks in the NLP field, which is limited in several directions.", "\u2022 We are testing defense strategies using downstream task models such as BERT and RoBERTa, and the purification tool is a model with a mask-filling ability such as BERT. Such a process can be further improved with strong models such as large language models.", "\u2022 We study the concept of adversarial purification in the adversarial attack scenarios with word-substitution attacks on small fine-tuned models. The concept of adversarial purification can be further expanded to various NLP applications. For instance, the purification of natural language can be used in malicious text purification which is more suitable in applications with large language models."], "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgement", "text": ["This work was supported by the National Natural Science Foundation of China (No. 62236004 and No. 62022027) and CAAI-Huawei MindSpore Open Fund."], "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Appendix Recovery Number Analysis", "text": ["One key problem is that how many recoveries we should use in the recovering process, as finding a proper T is also important in the image-domain purification process. We use two attack methods with K = 12 to test how the accuracy varies when using different recovery number N .", "As seen in Fig. 2 (a), the ensemble size is actually not a key factor. Larger ensemble size would not result in further improvements. We assume that larger ensemble size will smooth the output score which will benefit the attack algorithm. That is, the tiny difference between substitutes can be detected by the attack algorithm since the confidence score is given to the attack algorithms. Still, we can conclude that a multiple recovery process is effective in the purification process and quite simple to implement."], "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Candidate Size Analysis", "text": ["The attack algorithms such as BERT-Attack and Textfooler use a wide range of substitution set (e.g. K=50 in Textfooler means for each token to replace, the algorithm will find the best replacement in 50 candidates), which seriously harms the quality of the input texts.", "As seen in Fig. 2 (b), when the candidate is 0, the accuracy is high on the clean samples. When the candidate is 6, the normal fine-tuned BERT model cannot correctly predict the generated adversarial examples. This indicates that normal fine-tuned BERT is not robust even when the candidate size is small. After purification, the model can tolerate these limited candidate size attacks. When the candidate size grows, the performance of our defense framework drops by a relatively large margin. We assume that large candidate size would seriously harm the semantics which is also explored in Morris et al. (2020b), while these adversaries cannot be well evaluated even using human-evvaluations since the change rate is still low. B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?", "Left blank.", "B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Left blank. C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? Left blank."], "n_publication_ref": 1, "n_figure_ref": 1}], "references": [{"title": "Generating natural language adversarial examples", "journal": "CoRR", "year": "2018", "authors": "Moustafa Alzantot; Yash Sharma; Ahmed Elgohary; Bo-Jhang Ho; Mani B Srivastava; Kai-Wei Chang"}, {"title": "Towards evaluating the robustness of neural networks", "journal": "CoRR", "year": "2016", "authors": "Nicholas Carlini; David A Wagner"}, {"title": "Universal sentence encoder", "journal": "", "year": "2018", "authors": "Daniel Cer; Yinfei Yang; Sheng-Yi Kong; Nan Hua; Nicole Limtiaco; Rhomni St John; Noah Constant; Mario Guajardo-Cespedes; Steve Yuan; Chris Tar"}, {"title": "Robust neural machine translation with doubly adversarial inputs", "journal": "", "year": "2019", "authors": "Yong Cheng; Lu Jiang; Wolfgang Macherey"}, {"title": "BERT: pre-training of deep bidirectional transformers for language understanding", "journal": "CoRR", "year": "2018", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Towards robustness against natural language word substitutions", "journal": "", "year": "2021", "authors": "Xinshuai Dong; Hong Liu; Rongrong Ji; Anh Tuan Luu"}, {"title": "Hotflip: White-box adversarial examples for text classification", "journal": "", "year": "2017", "authors": "Javid Ebrahimi; Anyi Rao; Daniel Lowd; Dejing Dou"}, {"title": "Bae: Bert-based adversarial examples for text classification", "journal": "", "year": "2020", "authors": "Siddhant Garg; Goutham Ramakrishnan"}, {"title": "Explaining and harnessing adversarial examples", "journal": "", "year": "2014", "authors": "J Ian; Jonathon Goodfellow; Christian Shlens;  Szegedy"}, {"title": "Achieving verified robustness to symbol substitutions via interval bound propagation", "journal": "", "year": "2019", "authors": "Po-Sen Huang; Robert Stanforth; Johannes Welbl; Chris Dyer; Dani Yogatama; Sven Gowal; Krishnamurthy Dvijotham; Pushmeet Kohli"}, {"title": "Certified robustness to adversarial word substitutions", "journal": "", "year": "2019", "authors": "Robin Jia; Aditi Raghunathan; Kerem G\u00f6ksel; Percy Liang"}, {"title": "Is BERT really robust? natural language attack on text classification and entailment. CoRR, abs", "journal": "", "year": "1907", "authors": "Di Jin; Zhijing Jin; Joey Tianyi Zhou; Peter Szolovits"}, {"title": "A tutorial on energy-based learning", "journal": "", "year": "2006", "authors": "Yann Lecun; Sumit Chopra; Raia Hadsell; M Ranzato; F Huang"}, {"title": "Xiangyang Xue, and Xipeng Qiu. 2020. Bert-attack: Adversarial attack against bert using bert", "journal": "", "year": "", "authors": "Linyang Li; Ruotian Ma; Qipeng Guo"}, {"title": "Textat: Adversarial training for natural language understanding with token-level perturbation", "journal": "", "year": "2020", "authors": "Linyang Li; Xipeng Qiu"}, {"title": "Searching for an effective defender: Benchmarking defense against adversarial word substitution", "journal": "", "year": "2021", "authors": "Zongyi Li; Jianhan Xu; Jiehang Zeng; Linyang Li; Xiaoqing Zheng; Qi Zhang; Kai-Wei Chang; Cho-Jui Hsieh"}, {"title": "Roberta: A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "Learning word vectors for sentiment analysis", "journal": "", "year": "2011", "authors": "Andrew Maas; Raymond E Daly; T Peter; Dan Pham;  Huang; Y Andrew; Christopher Ng;  Potts"}, {"title": "Towards deep learning models resistant to adversarial attacks", "journal": "", "year": "2019", "authors": "Aleksander Madry; Aleksandar Makelov; Ludwig Schmidt; Dimitris Tsipras; Adrian Vladu"}, {"title": "Virtual adversarial training for semi-supervised text classification", "journal": "ArXiv", "year": "2016", "authors": "Takeru Miyato; Andrew M Dai; Ian J Goodfellow"}, {"title": "Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp", "journal": "", "year": "2020", "authors": "John Morris; Eli Lifland; Jin Yong Yoo; Jake Grigsby; Di Jin; Yanjun Qi"}, {"title": "Reevaluating adversarial examples in natural language", "journal": "", "year": "2004", "authors": "John X Morris; Eli Lifland; Jack Lanchantin; Yangfeng Ji; Yanjun Qi"}, {"title": "Counter-fitting word vectors to linguistic constraints", "journal": "", "year": "2016", "authors": "Nikola Mrk\u0161i\u0107; O Diarmuid; Blaise S\u00e9aghdha; Milica Thomson; Lina Ga\u0161i\u0107; Pei-Hao Rojas-Barahona; David Su; Tsung-Hsien Vandyke; Steve Wen;  Young"}, {"title": "Arash Vahdat, and Animashree Anandkumar. 2022. Diffusion models for adversarial purification", "journal": "PMLR", "year": "2022-07", "authors": "Weili Nie; Brandon Guo; Yujia Huang; Chaowei Xiao"}, {"title": "Generating natural language adversarial examples through probability weighted word saliency", "journal": "", "year": "2019", "authors": "Yihe Shuhuai Ren; Kun Deng; Wanxiang He;  Che"}, {"title": "Defense-gan: Protecting classifiers against adversarial attacks using generative models", "journal": "CoRR", "year": "2018", "authors": "Pouya Samangouei; Maya Kabkab; Rama Chellappa"}, {"title": "Online adversarial purification based on selfsupervised learning", "journal": "", "year": "2021-05-03", "authors": "Changhao Shi; Chester Holtz; Gal Mishne"}, {"title": "Robustness to modification with shared words in paraphrase identification", "journal": "", "year": "2019", "authors": "Zhouxing Shi; Minlie Huang; Ting Yao; Jingfang Xu"}, {"title": "Better robustness by more coverage: Adversarial training with mixup augmentation for robust fine-tuning", "journal": "", "year": "2020", "authors": "Chenglei Si; Zhengyan Zhang; Fanchao Qi; Zhiyuan Liu; Yasheng Wang; Qun Liu; Maosong Sun"}, {"title": "Score-based generative modeling through stochastic differential equations", "journal": "", "year": "2021-05-03", "authors": "Yang Song; Jascha Sohl-Dickstein; Diederik P Kingma; Abhishek Kumar; Stefano Ermon; Ben Poole"}, {"title": "Robustifying models against adversarial attacks by langevin dynamics", "journal": "Neural Networks", "year": "2021", "authors": "Vignesh Srinivasan; Csaba Rohrer; Arturo Marb\u00e1n; Klaus-Robert M\u00fcller; Wojciech Samek; Shinichi Nakajima"}, {"title": "Transformers: State-of-the-art natural language processing", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger; Quentin Drame; Alexander M Lhoest;  Rush"}, {"title": "Searching for a search method: Benchmarking search algorithms for generating nlp adversarial examples", "journal": "", "year": "2009", "authors": "Jin Yong Yoo; John X Morris; Eli Lifland; Yanjun Qi"}, {"title": "Adversarial purification with score-based generative models", "journal": "PMLR", "year": "2021-07-24", "authors": "Jongmin Yoon; Sung Ju Hwang; Juho Lee"}, {"title": "Certified robustness to text adversarial attacks by randomized", "journal": "", "year": "2021", "authors": "Jiehang Zeng; Xiaoqing Zheng; Jianhan Xu; Linyang Li; Liping Yuan; Xuanjing Huang"}, {"title": "Character-level convolutional networks for text classification", "journal": "", "year": "2015", "authors": "Xiang Zhang; Junbo Zhao; Yann Lecun"}, {"title": "Defense against adversarial attacks in nlp via dirichlet neighborhood ensemble", "journal": "", "year": "2020", "authors": "Yi Zhou; Xiaoqing Zheng; Cho-Jui Hsieh; Kai-Wei Chang; Xuanjing Huang"}, {"title": "Freelb: Enhanced adversarial training for language understanding", "journal": "", "year": "2019", "authors": "Chen Zhu; Yu Cheng; Zhe Gan; Siqi Sun; Thomas Goldstein; Jingjing Liu"}, {"title": "Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?", "journal": "", "year": "", "authors": ""}, {"title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run", "journal": "", "year": "", "authors": ""}, {"title": "for preprocessing, for normalization, or for evaluation", "journal": "", "year": "", "authors": " Nltk;  Spacy;  Rouge"}, {"title": "crowdworkers) or research with human participants? Left blank", "journal": "", "year": "", "authors": ""}, {"title": "Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators", "journal": "", "year": "", "authors": " D1"}, {"title": "crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic", "journal": "", "year": "", "authors": ""}, {"title": "Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?", "journal": "", "year": "", "authors": " D3"}, {"title": "Was the data collection protocol approved (or determined exempt) by an ethics review board?", "journal": "", "year": "", "authors": " D4"}, {"title": "Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?", "journal": "", "year": "", "authors": " D5"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: Text Adversarial Purification Process: Compared with Image Purification, we use masked language models to recover noisy texts to purify adversarial texts as a defense against word-substitutions attacks.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_2", "figure_caption": "4 https://github.com/huggingface/transformers use the model that is trained with gradient-based adversarial training as the purification model F m (\u2022) and the classifier F c (\u2022) for the main experiments and conduct thorough ablations to explore the effect of combining purification with classifier and adversarially trained classifier.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "After-Attack Accuracy compared with defense methods that can defend attacks without acknowledging the form of the attacks. That is, the substitution candidates of the attack methods are unknown to defense systems.", "figure_data": "MethodsOrigin Textfooler GAIMDB \u2193 BERT \u25a0 Data-Augmentation94.0 93.02.0 18.045.0 53.0\u2022ADA"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "After-Attack Accuracy compared with accesscandidates methods based on the BERT model. Here we implement Textfooler with K=50 for consistency with previous works. GA is the Genetic Attack method. We use the AMDA-SMix setup for the AMDA method.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Ablations results tested on attacking the IMDB task based on BERT models. Comb. Classifier is the combined fine-tuned F", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "I've been an amateur consumers of newly technologies and I kept working out of them on skateboards ! well ... 81.4% I know the wonderfully general sense notion to knows that oils can not last endless and I am acutely know of how majority part of my lived in the big city spins around petrocochemical production ... I should have been an amateur consumers fan of newly technologies and I kept on working out of spaces and on powerboards ! well ... 76.2% I am the the general sense notion and knows that oils can not last endless and I am acutely know of the part of my lived as the city spins around petrochemical production ... I've been an amateur consumers of newly technologies and I kept working out of bed on powerboards ! well ... 78.5%", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "A random selected sample that BERT model failed to defend against the Textfooler Attack in the IMDB dataset and Text Pure (Text Adversarial Purification) succeed. Adv. of BERT is the adversarial sample generated by Textfooler to attack the classifier. Adv. of Text Pure is the sample generated by Textfooler to attack the classifier but failed. The purified texts are also listed.", "figure_data": ""}], "formulas": [], "doi": "10.1016/j.neunet.2020.12.024"}